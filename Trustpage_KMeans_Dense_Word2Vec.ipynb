{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dense dataset using Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '.\\IMDB_Dataset.csv'\n",
    "imdb = pd.read_csv(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "#     text = text.split()\n",
    "    text = str(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the text prep to each row of the data frame\n",
    "imdb.review = imdb.review.apply(lambda x: text_to_word_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating label for evaluation\n",
    "imdb['sentiment'] = imdb['sentiment'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there a family where a little boy ja...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love in the time of money is a v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one of the other reviewers has mentioned that ...          1\n",
       "1  a wonderful little production the filming tech...          1\n",
       "2  i thought this was a wonderful way to spend ti...          1\n",
       "3  basically there a family where a little boy ja...          0\n",
       "4  petter mattei love in the time of money is a v...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to tokenize the text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\STaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\STaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\STaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt');\n",
    "nltk.download('stopwords');\n",
    "nltk.download('wordnet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep word tokenize the text (lemmatize)\n",
    "# def prep_word (text):\n",
    "#     sw = set(stopwords.words(\"english\"))\n",
    "#     word_tk = word_tokenize(text)\n",
    "#     words_nonstop = [w for w in word_tk if not w in sw]\n",
    "#     port_stem = PorterStemmer()\n",
    "#     lem = WordNetLemmatizer()\n",
    "#     lemm_words = []\n",
    "#     stemmed_word = []\n",
    "#     for w in words_nonstop:\n",
    "#         stemmed_word.append(port_stem.stem(w))\n",
    "#     return stemmed_word\n",
    "\n",
    "# from gensim.models.phrases import Phrases, Phraser\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.test.utils import get_tmpfile\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# sent = [row for row in imdb.review]\n",
    "# phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "# bigram = Phraser(phrases)\n",
    "# sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined seveal word tokenizing function as seen above, and compared their \n",
    "stop_words = set(['all', \"she'll\", \"don't\", 'being', 'over', 'through', \n",
    "'yourselves', 'its', 'before', \"he's\", \"when's\", \"we've\", 'had', 'should',\n",
    "\"he'd\", 'to', 'only', \"there's\", 'those', 'under', 'ours', 'has', \n",
    "\"haven't\", 'do', 'them', 'his', \"they'll\", 'very', \"who's\", \"they'd\", \n",
    "'cannot', \"you've\", 'they', 'not', 'during', 'yourself', 'him', 'nor', \n",
    "\"we'll\", 'did', \"they've\", 'this', 'she', 'each', \"won't\", 'where', \n",
    "\"mustn't\", \"isn't\", \"i'll\", \"why's\", 'because', \"you'd\", 'doing', 'some', \n",
    "'up', 'are', 'further', 'ourselves', 'out', 'what', 'for', 'while', \n",
    "\"wasn't\", 'does', \"shouldn't\", 'above', 'between', 'be', 'we', 'who', \n",
    "\"you're\", 'were', 'here', 'hers', \"aren't\", 'by', 'both', 'about', 'would', \n",
    "'of', 'could', 'against', \"i'd\", \"weren't\", \"i'm\", 'or', \"can't\", 'own', \n",
    "'into', 'whom', 'down', \"hadn't\", \"couldn't\", 'your', \"doesn't\", 'from', \n",
    "\"how's\", 'her', 'their', \"it's\", 'there', 'been', 'why', 'few', 'too', \n",
    "'themselves', 'was', 'until', 'more', 'himself', \"where's\", \"i've\", 'with', \n",
    "\"didn't\", \"what's\", 'but', 'herself', 'than', \"here's\", 'he', 'me', \n",
    "\"they're\", 'myself', 'these', \"hasn't\", 'below', 'ought', 'theirs', 'my', \n",
    "\"wouldn't\", \"we'd\", 'and', 'then', 'is', 'am', 'it', 'an', 'as', 'itself', \n",
    "'at', 'have', 'in', 'any', 'if', 'again', 'no', 'that', 'when', 'same', \n",
    "'how', 'other', 'which', 'you', \"shan't\", 'our', 'after', \"let's\", 'most', \n",
    "'such', 'on', \"he'll\", 'a', 'off', 'i', \"she'd\", 'yours', \"you'll\", 'so', \n",
    "\"we're\", \"she's\", 'the', \"that's\", 'having', 'once'])\n",
    "def tokenize(texts):\n",
    "    compiler = re.compile('[\\W_]+', re.UNICODE)\n",
    "    sentences = []\n",
    "    for text in texts:\n",
    "        sentence = text.lower().split(\" \")\n",
    "        sentence = [compiler.sub('', w) for w in sentence]\n",
    "        sentences.append( [w for w in sentence if w not in stop_words] )\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenization to each row of the data frame\n",
    "sentences = tokenize(imdb.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewers',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " 'just',\n",
       " '1',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'will',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly',\n",
       " 'happened',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'oz',\n",
       " 'brutality',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'violence',\n",
       " 'set',\n",
       " 'right',\n",
       " 'word',\n",
       " 'go',\n",
       " 'trust',\n",
       " 'show',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'timid',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'punches',\n",
       " 'regards',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'violence',\n",
       " 'hardcore',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'word',\n",
       " 'called',\n",
       " 'oz',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'oswald',\n",
       " 'maximum',\n",
       " 'security',\n",
       " 'state',\n",
       " 'penitentary',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'emerald',\n",
       " 'city',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'prison',\n",
       " 'cells',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'privacy',\n",
       " 'high',\n",
       " 'agenda',\n",
       " 'em',\n",
       " 'city',\n",
       " 'home',\n",
       " 'many',\n",
       " 'aryans',\n",
       " 'muslims',\n",
       " 'gangstas',\n",
       " 'latinos',\n",
       " 'christians',\n",
       " 'italians',\n",
       " 'irish',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'say',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'show',\n",
       " 'due',\n",
       " 'fact',\n",
       " 'goes',\n",
       " 'shows',\n",
       " 'dare',\n",
       " 'forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romance',\n",
       " 'oz',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'nasty',\n",
       " 'surreal',\n",
       " 'say',\n",
       " 'ready',\n",
       " 'watched',\n",
       " 'developed',\n",
       " 'taste',\n",
       " 'oz',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'just',\n",
       " 'violence',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'will',\n",
       " 'sold',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'will',\n",
       " 'kill',\n",
       " 'order',\n",
       " 'get',\n",
       " 'away',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'turned',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'uncomfortable',\n",
       " 'viewing',\n",
       " 'thats',\n",
       " 'can',\n",
       " 'get',\n",
       " 'touch',\n",
       " 'darker',\n",
       " 'side',\n",
       " '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STaine\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     vector_size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70501308, 188819400)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model for 30 epoches on entire dataset\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "word_vectors = Word2Vec.load(\".\\word2vec.model\").wv\n",
    "model =  Word2Vec.load(\".\\word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''in order to feed the data into the Kmeans, created \n",
    "a function to average each row over every single word, as it  \n",
    "is represented by a vector of size vector_size'''\n",
    "\n",
    "import numpy as np\n",
    "def ave_w2v(model, sentences):\n",
    "    ave_f= np.zeros((len(sentences), model.vector_size))\n",
    "    for i, sent in enumerate(sentences):\n",
    "        for word in sent:\n",
    "            try:\n",
    "                vector = model[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        ave_f[i,:] = ave_f[i,:] + vector\n",
    "        ave_f[i,:] = ave_f[i,:] / len(sent)\n",
    "    return ave_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the original code to run with K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging the words vector across each row\n",
    "# ave_w2v = ave_w2v(model, sentences)\n",
    "\n",
    "# KMeans clustering with max_iter 1000\n",
    "from sklearn.cluster import KMeans\n",
    "KMeans = KMeans(n_clusters=3, max_iter=1000, algorithm = 'auto')\n",
    "\n",
    "fitted = KMeans.fit(word_vectors.vectors.astype('double'))\n",
    "prediction = KMeans.predict(word_vectors.vectors.astype('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.40\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score\n",
    "imdb['sentiment_pred'] = pd.Series(prediction)\n",
    "# f1 = f1_score(imdb['sentiment'],imdb['sentiment_pred'],pos_label=1)\n",
    "acc = accuracy_score(imdb['sentiment'],imdb['sentiment_pred'])\n",
    "\n",
    "print(\"Accuracy {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning each word the cluster its own cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model performance by trying KMeans for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the cluster centers appropirate values\n",
    "positive = 1\n",
    "positive_center = KMeans.cluster_centers_[positive]\n",
    "negative_center = KMeans.cluster_centers_[1-positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50939, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining the shpae of the vector out of word2vec\n",
    "word_vectors.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.194555</td>\n",
       "      <td>0.283483</td>\n",
       "      <td>0.206052</td>\n",
       "      <td>0.195558</td>\n",
       "      <td>-0.098831</td>\n",
       "      <td>-0.118183</td>\n",
       "      <td>0.142301</td>\n",
       "      <td>0.297484</td>\n",
       "      <td>-0.449390</td>\n",
       "      <td>-0.034129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023032</td>\n",
       "      <td>0.499222</td>\n",
       "      <td>0.348928</td>\n",
       "      <td>0.152259</td>\n",
       "      <td>0.160356</td>\n",
       "      <td>0.169473</td>\n",
       "      <td>0.214394</td>\n",
       "      <td>-0.342808</td>\n",
       "      <td>0.023434</td>\n",
       "      <td>-0.412089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.140590</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>-0.222870</td>\n",
       "      <td>0.329827</td>\n",
       "      <td>-0.224997</td>\n",
       "      <td>-0.120110</td>\n",
       "      <td>0.102595</td>\n",
       "      <td>0.843669</td>\n",
       "      <td>-0.294650</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229324</td>\n",
       "      <td>0.348344</td>\n",
       "      <td>0.430058</td>\n",
       "      <td>0.268635</td>\n",
       "      <td>0.595714</td>\n",
       "      <td>0.250054</td>\n",
       "      <td>-0.220882</td>\n",
       "      <td>-0.096698</td>\n",
       "      <td>0.106332</td>\n",
       "      <td>-0.566308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.091227</td>\n",
       "      <td>0.102367</td>\n",
       "      <td>-0.448254</td>\n",
       "      <td>0.266532</td>\n",
       "      <td>-0.280452</td>\n",
       "      <td>-0.038558</td>\n",
       "      <td>0.044803</td>\n",
       "      <td>0.031807</td>\n",
       "      <td>-0.228689</td>\n",
       "      <td>0.259630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291684</td>\n",
       "      <td>0.308395</td>\n",
       "      <td>0.261651</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>0.293836</td>\n",
       "      <td>0.206790</td>\n",
       "      <td>0.122776</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.161961</td>\n",
       "      <td>-0.473563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.087425</td>\n",
       "      <td>-0.025102</td>\n",
       "      <td>0.311978</td>\n",
       "      <td>0.431233</td>\n",
       "      <td>-0.191786</td>\n",
       "      <td>-0.133311</td>\n",
       "      <td>-0.105742</td>\n",
       "      <td>0.323807</td>\n",
       "      <td>-0.030460</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143623</td>\n",
       "      <td>-0.220800</td>\n",
       "      <td>0.370557</td>\n",
       "      <td>0.188152</td>\n",
       "      <td>0.469862</td>\n",
       "      <td>0.290857</td>\n",
       "      <td>0.114241</td>\n",
       "      <td>0.139850</td>\n",
       "      <td>-0.019146</td>\n",
       "      <td>-0.370761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.360749</td>\n",
       "      <td>0.758457</td>\n",
       "      <td>-0.191592</td>\n",
       "      <td>-0.257975</td>\n",
       "      <td>0.102662</td>\n",
       "      <td>-0.103710</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>0.303837</td>\n",
       "      <td>0.058728</td>\n",
       "      <td>-0.371631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066145</td>\n",
       "      <td>-0.186011</td>\n",
       "      <td>-0.132846</td>\n",
       "      <td>0.159813</td>\n",
       "      <td>0.099227</td>\n",
       "      <td>-0.268661</td>\n",
       "      <td>-0.111887</td>\n",
       "      <td>-0.025944</td>\n",
       "      <td>-0.459700</td>\n",
       "      <td>0.283484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.194555  0.283483  0.206052  0.195558 -0.098831 -0.118183  0.142301   \n",
       "1 -0.140590  0.570906 -0.222870  0.329827 -0.224997 -0.120110  0.102595   \n",
       "2 -0.091227  0.102367 -0.448254  0.266532 -0.280452 -0.038558  0.044803   \n",
       "3 -0.087425 -0.025102  0.311978  0.431233 -0.191786 -0.133311 -0.105742   \n",
       "4 -0.360749  0.758457 -0.191592 -0.257975  0.102662 -0.103710 -0.230652   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  0.297484 -0.449390 -0.034129  ...  0.023032  0.499222  0.348928  0.152259   \n",
       "1  0.843669 -0.294650  0.006930  ... -0.229324  0.348344  0.430058  0.268635   \n",
       "2  0.031807 -0.228689  0.259630  ... -0.291684  0.308395  0.261651  0.009782   \n",
       "3  0.323807 -0.030460 -0.243507  ... -0.143623 -0.220800  0.370557  0.188152   \n",
       "4  0.303837  0.058728 -0.371631  ...  0.066145 -0.186011 -0.132846  0.159813   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.160356  0.169473  0.214394 -0.342808  0.023434 -0.412089  \n",
       "1  0.595714  0.250054 -0.220882 -0.096698  0.106332 -0.566308  \n",
       "2  0.293836  0.206790  0.122776  0.037662  0.161961 -0.473563  \n",
       "3  0.469862  0.290857  0.114241  0.139850 -0.019146 -0.370761  \n",
       "4  0.099227 -0.268661 -0.111887 -0.025944 -0.459700  0.283484  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating words datafame from word_vector that is created from word2vec\n",
    "words = pd.DataFrame(word_vectors.vectors)\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the key for each word, and putting all vectors in one column for each word\n",
    "words['words'] = word_vectors.index_to_key\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "\n",
    "# using the KMeans predict each word cluster and assigning 1 or -1 for each cluster\n",
    "words['cluster'] = words.vectors.apply(lambda x: KMeans.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])\n",
    "words['cluster_number'] = [1 if i==positive  else -1 for i in words.cluster]\n",
    "\n",
    "# kmeans.transform(X) returns is already the L2 norm distance to each cluster center,thus a measure of how \n",
    "# accurate or closeness the word to the cluster\n",
    "words['l2_distance'] = words.apply(lambda x: 1/(KMeans.transform([x.vectors]).min()), axis=1)\n",
    "\n",
    "# calculating score for each word based on their distance to the center, negative number will \n",
    "# be from cluster 0 and positive number from cluster 1\n",
    "words['word_score'] = words.l2_distance * words.cluster_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>l2_distance</th>\n",
       "      <th>word_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.194555</td>\n",
       "      <td>0.283483</td>\n",
       "      <td>0.206052</td>\n",
       "      <td>0.195558</td>\n",
       "      <td>-0.098831</td>\n",
       "      <td>-0.118183</td>\n",
       "      <td>0.142301</td>\n",
       "      <td>0.297484</td>\n",
       "      <td>-0.449390</td>\n",
       "      <td>-0.034129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214394</td>\n",
       "      <td>-0.342808</td>\n",
       "      <td>0.023434</td>\n",
       "      <td>-0.412089</td>\n",
       "      <td></td>\n",
       "      <td>[-0.19455506, 0.28348318, 0.20605221, 0.195558...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.241723</td>\n",
       "      <td>-0.241723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.140590</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>-0.222870</td>\n",
       "      <td>0.329827</td>\n",
       "      <td>-0.224997</td>\n",
       "      <td>-0.120110</td>\n",
       "      <td>0.102595</td>\n",
       "      <td>0.843669</td>\n",
       "      <td>-0.294650</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220882</td>\n",
       "      <td>-0.096698</td>\n",
       "      <td>0.106332</td>\n",
       "      <td>-0.566308</td>\n",
       "      <td>movie</td>\n",
       "      <td>[-0.14058958, 0.5709055, -0.22287044, 0.329827...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.172073</td>\n",
       "      <td>0.172073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.091227</td>\n",
       "      <td>0.102367</td>\n",
       "      <td>-0.448254</td>\n",
       "      <td>0.266532</td>\n",
       "      <td>-0.280452</td>\n",
       "      <td>-0.038558</td>\n",
       "      <td>0.044803</td>\n",
       "      <td>0.031807</td>\n",
       "      <td>-0.228689</td>\n",
       "      <td>0.259630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122776</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.161961</td>\n",
       "      <td>-0.473563</td>\n",
       "      <td>film</td>\n",
       "      <td>[-0.091226876, 0.10236713, -0.44825375, 0.2665...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.243680</td>\n",
       "      <td>0.243680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.087425</td>\n",
       "      <td>-0.025102</td>\n",
       "      <td>0.311978</td>\n",
       "      <td>0.431233</td>\n",
       "      <td>-0.191786</td>\n",
       "      <td>-0.133311</td>\n",
       "      <td>-0.105742</td>\n",
       "      <td>0.323807</td>\n",
       "      <td>-0.030460</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114241</td>\n",
       "      <td>0.139850</td>\n",
       "      <td>-0.019146</td>\n",
       "      <td>-0.370761</td>\n",
       "      <td>one</td>\n",
       "      <td>[-0.08742523, -0.025102125, 0.31197816, 0.4312...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.215025</td>\n",
       "      <td>0.215025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.360749</td>\n",
       "      <td>0.758457</td>\n",
       "      <td>-0.191592</td>\n",
       "      <td>-0.257975</td>\n",
       "      <td>0.102662</td>\n",
       "      <td>-0.103710</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>0.303837</td>\n",
       "      <td>0.058728</td>\n",
       "      <td>-0.371631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111887</td>\n",
       "      <td>-0.025944</td>\n",
       "      <td>-0.459700</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>like</td>\n",
       "      <td>[-0.36074874, 0.7584568, -0.19159229, -0.25797...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.197997</td>\n",
       "      <td>-0.197997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.194555  0.283483  0.206052  0.195558 -0.098831 -0.118183  0.142301   \n",
       "1 -0.140590  0.570906 -0.222870  0.329827 -0.224997 -0.120110  0.102595   \n",
       "2 -0.091227  0.102367 -0.448254  0.266532 -0.280452 -0.038558  0.044803   \n",
       "3 -0.087425 -0.025102  0.311978  0.431233 -0.191786 -0.133311 -0.105742   \n",
       "4 -0.360749  0.758457 -0.191592 -0.257975  0.102662 -0.103710 -0.230652   \n",
       "\n",
       "          7         8         9  ...       296       297       298       299  \\\n",
       "0  0.297484 -0.449390 -0.034129  ...  0.214394 -0.342808  0.023434 -0.412089   \n",
       "1  0.843669 -0.294650  0.006930  ... -0.220882 -0.096698  0.106332 -0.566308   \n",
       "2  0.031807 -0.228689  0.259630  ...  0.122776  0.037662  0.161961 -0.473563   \n",
       "3  0.323807 -0.030460 -0.243507  ...  0.114241  0.139850 -0.019146 -0.370761   \n",
       "4  0.303837  0.058728 -0.371631  ... -0.111887 -0.025944 -0.459700  0.283484   \n",
       "\n",
       "   words                                            vectors  cluster  \\\n",
       "0         [-0.19455506, 0.28348318, 0.20605221, 0.195558...        0   \n",
       "1  movie  [-0.14058958, 0.5709055, -0.22287044, 0.329827...        1   \n",
       "2   film  [-0.091226876, 0.10236713, -0.44825375, 0.2665...        1   \n",
       "3    one  [-0.08742523, -0.025102125, 0.31197816, 0.4312...        1   \n",
       "4   like  [-0.36074874, 0.7584568, -0.19159229, -0.25797...        0   \n",
       "\n",
       "   cluster_number  l2_distance  word_score  \n",
       "0              -1     0.241723   -0.241723  \n",
       "1               1     0.172073    0.172073  \n",
       "2               1     0.243680    0.243680  \n",
       "3               1     0.215025    0.215025  \n",
       "4              -1     0.197997   -0.197997  \n",
       "\n",
       "[5 rows x 306 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retaining the needed columns and store them into a new data frame\n",
    "words_trimmed = words[['words', 'vectors', 'cluster', 'cluster_number','l2_distance', 'word_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the words columns as index\n",
    "words_trimmed.set_index(words.words, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>l2_distance</th>\n",
       "      <th>word_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td>[-0.19455506, 0.28348318, 0.20605221, 0.195558...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.241723</td>\n",
       "      <td>-0.241723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>movie</td>\n",
       "      <td>[-0.14058958, 0.5709055, -0.22287044, 0.329827...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.172073</td>\n",
       "      <td>0.172073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>film</td>\n",
       "      <td>[-0.091226876, 0.10236713, -0.44825375, 0.2665...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.243680</td>\n",
       "      <td>0.243680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>one</td>\n",
       "      <td>[-0.08742523, -0.025102125, 0.31197816, 0.4312...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.215025</td>\n",
       "      <td>0.215025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>like</td>\n",
       "      <td>[-0.36074874, 0.7584568, -0.19159229, -0.25797...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.197997</td>\n",
       "      <td>-0.197997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>just</td>\n",
       "      <td>[-0.24510749, 0.38719353, -0.011783284, 0.0007...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.190104</td>\n",
       "      <td>0.190104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>good</td>\n",
       "      <td>[0.19814792, 0.8872247, 0.17918931, 0.14578643...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.174220</td>\n",
       "      <td>0.174220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>time</td>\n",
       "      <td>[-0.18041916, 0.3175915, 0.27195337, 0.1915056...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177395</td>\n",
       "      <td>0.177395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>even</td>\n",
       "      <td>[-0.04309079, 0.20007537, -0.34513903, -0.3730...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194733</td>\n",
       "      <td>0.194733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>will</td>\n",
       "      <td>[-0.20239432, 0.4686843, 0.40891212, 0.2796957...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.155112</td>\n",
       "      <td>-0.155112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>story</td>\n",
       "      <td>[0.051561855, 0.11486601, 0.11613593, 0.005188...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163962</td>\n",
       "      <td>0.163962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>really</td>\n",
       "      <td>[-0.15956417, 0.5350447, -0.18707435, -0.09774...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186785</td>\n",
       "      <td>0.186785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>see</td>\n",
       "      <td>[-0.33177364, 0.57119185, 0.115189485, 0.00997...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186898</td>\n",
       "      <td>0.186898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>can</td>\n",
       "      <td>[-0.045773186, 0.48658392, -0.1992637, -0.1158...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188658</td>\n",
       "      <td>0.188658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>well</td>\n",
       "      <td>[0.19789116, 0.46953967, -0.010101889, 0.27403...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181949</td>\n",
       "      <td>0.181949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words                                            vectors  cluster  \\\n",
       "words                                                                        \n",
       "                [-0.19455506, 0.28348318, 0.20605221, 0.195558...        0   \n",
       "movie    movie  [-0.14058958, 0.5709055, -0.22287044, 0.329827...        1   \n",
       "film      film  [-0.091226876, 0.10236713, -0.44825375, 0.2665...        1   \n",
       "one        one  [-0.08742523, -0.025102125, 0.31197816, 0.4312...        1   \n",
       "like      like  [-0.36074874, 0.7584568, -0.19159229, -0.25797...        0   \n",
       "just      just  [-0.24510749, 0.38719353, -0.011783284, 0.0007...        1   \n",
       "good      good  [0.19814792, 0.8872247, 0.17918931, 0.14578643...        1   \n",
       "time      time  [-0.18041916, 0.3175915, 0.27195337, 0.1915056...        1   \n",
       "even      even  [-0.04309079, 0.20007537, -0.34513903, -0.3730...        1   \n",
       "will      will  [-0.20239432, 0.4686843, 0.40891212, 0.2796957...        0   \n",
       "story    story  [0.051561855, 0.11486601, 0.11613593, 0.005188...        1   \n",
       "really  really  [-0.15956417, 0.5350447, -0.18707435, -0.09774...        1   \n",
       "see        see  [-0.33177364, 0.57119185, 0.115189485, 0.00997...        1   \n",
       "can        can  [-0.045773186, 0.48658392, -0.1992637, -0.1158...        1   \n",
       "well      well  [0.19789116, 0.46953967, -0.010101889, 0.27403...        1   \n",
       "\n",
       "        cluster_number  l2_distance  word_score  \n",
       "words                                            \n",
       "                    -1     0.241723   -0.241723  \n",
       "movie                1     0.172073    0.172073  \n",
       "film                 1     0.243680    0.243680  \n",
       "one                  1     0.215025    0.215025  \n",
       "like                -1     0.197997   -0.197997  \n",
       "just                 1     0.190104    0.190104  \n",
       "good                 1     0.174220    0.174220  \n",
       "time                 1     0.177395    0.177395  \n",
       "even                 1     0.194733    0.194733  \n",
       "will                -1     0.155112   -0.155112  \n",
       "story                1     0.163962    0.163962  \n",
       "really               1     0.186785    0.186785  \n",
       "see                  1     0.186898    0.186898  \n",
       "can                  1     0.188658    0.188658  \n",
       "well                 1     0.181949    0.181949  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_trimmed.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having three clusters, inspecting each cluster for the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    31109\n",
       "1     9923\n",
       "2     9907\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_trimmed['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words\n",
       "movie              movie\n",
       "film                film\n",
       "one                  one\n",
       "just                just\n",
       "good                good\n",
       "time                time\n",
       "even                even\n",
       "story              story\n",
       "really            really\n",
       "see                  see\n",
       "can                  can\n",
       "well                well\n",
       "much                much\n",
       "people            people\n",
       "great              great\n",
       "also                also\n",
       "made                made\n",
       "make                make\n",
       "way                  way\n",
       "movies            movies\n",
       "characters    characters\n",
       "think              think\n",
       "watch              watch\n",
       "films              films\n",
       "many                many\n",
       "seen                seen\n",
       "love                love\n",
       "never              never\n",
       "plot                plot\n",
       "life                life\n",
       "acting            acting\n",
       "show                show\n",
       "best                best\n",
       "little            little\n",
       "ever                ever\n",
       "better            better\n",
       "end                  end\n",
       "still              still\n",
       "say                  say\n",
       "scenes            scenes\n",
       "something      something\n",
       "real                real\n",
       "thing              thing\n",
       "watching        watching\n",
       "actors            actors\n",
       "director        director\n",
       "funny              funny\n",
       "though            though\n",
       "work                work\n",
       "10                    10\n",
       "Name: words, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_trimmed['words'][words_trimmed['cluster']==1].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizng stop words based on the new clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_k0 = words_trimmed['words'][words_trimmed['cluster']==0]\n",
    "stop_words_k1 = words_trimmed['words'][words_trimmed['cluster']==1]\n",
    "stop_words_k2 = words_trimmed['words'][words_trimmed['cluster']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating variables across notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'stop_words_k0' (Series)\n",
      "Stored 'stop_words_k1' (Series)\n",
      "Stored 'stop_words_k2' (Series)\n"
     ]
    }
   ],
   "source": [
    "%store stop_words_k0\n",
    "%store stop_words_k1\n",
    "%store stop_words_k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I transfer these cutomized stop words to the Sparse data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating an average score for each row based on its words scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = np.zeros(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function iterate through each sentence, and grabs their word score as defined above\n",
    "# then average over the length of that sentence to calculate a value which is an average score value\n",
    "# of all the words\n",
    "for i, row in enumerate(sentences):\n",
    "    if i%100 == 0:\n",
    "        print('iteration ', i)\n",
    "    row_list = []\n",
    "    for sent in row:\n",
    "        if words_trimmed['words'].str.contains(sent).any():\n",
    "            try:\n",
    "                row_list.append(words_trimmed.loc[sent]['word_score'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "    new_pred[i] = (np.mean(row_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the predictoin into a new column\n",
    "imdb['new_pred']=pd.Series(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb['new_pred'] = [1 if i > 0 else 0 for i in imdb.new_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_final = imdb.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score\n",
    "f1 = f1_score(imdb_final['sentiment'],imdb_final['new_pred'],pos_label=1)\n",
    "acc = accuracy_score(imdb_final['sentiment'],imdb_final['new_pred'])\n",
    "\n",
    "print(\"Accuracy {:.2f}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
