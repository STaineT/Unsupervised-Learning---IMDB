{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dense dataset using Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '.\\IMDB_Dataset.csv'\n",
    "imdb = pd.read_csv(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "#     text = text.split()\n",
    "    text = str(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the text prep to each row of the data frame\n",
    "imdb.review = imdb.review.apply(lambda x: text_to_word_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating label for evaluation\n",
    "imdb['sentiment'] = imdb['sentiment'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there a family where a little boy ja...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love in the time of money is a v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one of the other reviewers has mentioned that ...          1\n",
       "1  a wonderful little production the filming tech...          1\n",
       "2  i thought this was a wonderful way to spend ti...          1\n",
       "3  basically there a family where a little boy ja...          0\n",
       "4  petter mattei love in the time of money is a v...          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to tokenize the text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\STaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\STaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\STaine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt');\n",
    "nltk.download('stopwords');\n",
    "nltk.download('wordnet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep word tokenize the text (lemmatize)\n",
    "# def prep_word (text):\n",
    "#     sw = set(stopwords.words(\"english\"))\n",
    "#     word_tk = word_tokenize(text)\n",
    "#     words_nonstop = [w for w in word_tk if not w in sw]\n",
    "#     port_stem = PorterStemmer()\n",
    "#     lem = WordNetLemmatizer()\n",
    "#     lemm_words = []\n",
    "#     stemmed_word = []\n",
    "#     for w in words_nonstop:\n",
    "#         stemmed_word.append(port_stem.stem(w))\n",
    "#     return stemmed_word\n",
    "\n",
    "# from gensim.models.phrases import Phrases, Phraser\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.test.utils import get_tmpfile\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# sent = [row for row in imdb.review]\n",
    "# phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "# bigram = Phraser(phrases)\n",
    "# sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined seveal word tokenizing function as seen above, and compared their \n",
    "stop_words = set(['all', \"she'll\", \"don't\", 'being', 'over', 'through', \n",
    "'yourselves', 'its', 'before', \"he's\", \"when's\", \"we've\", 'had', 'should',\n",
    "\"he'd\", 'to', 'only', \"there's\", 'those', 'under', 'ours', 'has', \n",
    "\"haven't\", 'do', 'them', 'his', \"they'll\", 'very', \"who's\", \"they'd\", \n",
    "'cannot', \"you've\", 'they', 'not', 'during', 'yourself', 'him', 'nor', \n",
    "\"we'll\", 'did', \"they've\", 'this', 'she', 'each', \"won't\", 'where', \n",
    "\"mustn't\", \"isn't\", \"i'll\", \"why's\", 'because', \"you'd\", 'doing', 'some', \n",
    "'up', 'are', 'further', 'ourselves', 'out', 'what', 'for', 'while', \n",
    "\"wasn't\", 'does', \"shouldn't\", 'above', 'between', 'be', 'we', 'who', \n",
    "\"you're\", 'were', 'here', 'hers', \"aren't\", 'by', 'both', 'about', 'would', \n",
    "'of', 'could', 'against', \"i'd\", \"weren't\", \"i'm\", 'or', \"can't\", 'own', \n",
    "'into', 'whom', 'down', \"hadn't\", \"couldn't\", 'your', \"doesn't\", 'from', \n",
    "\"how's\", 'her', 'their', \"it's\", 'there', 'been', 'why', 'few', 'too', \n",
    "'themselves', 'was', 'until', 'more', 'himself', \"where's\", \"i've\", 'with', \n",
    "\"didn't\", \"what's\", 'but', 'herself', 'than', \"here's\", 'he', 'me', \n",
    "\"they're\", 'myself', 'these', \"hasn't\", 'below', 'ought', 'theirs', 'my', \n",
    "\"wouldn't\", \"we'd\", 'and', 'then', 'is', 'am', 'it', 'an', 'as', 'itself', \n",
    "'at', 'have', 'in', 'any', 'if', 'again', 'no', 'that', 'when', 'same', \n",
    "'how', 'other', 'which', 'you', \"shan't\", 'our', 'after', \"let's\", 'most', \n",
    "'such', 'on', \"he'll\", 'a', 'off', 'i', \"she'd\", 'yours', \"you'll\", 'so', \n",
    "\"we're\", \"she's\", 'the', \"that's\", 'having', 'once'])\n",
    "def tokenize(texts):\n",
    "    compiler = re.compile('[\\W_]+', re.UNICODE)\n",
    "    sentences = []\n",
    "    for text in texts:\n",
    "        sentence = text.lower().split(\" \")\n",
    "        sentence = [compiler.sub('', w) for w in sentence]\n",
    "        sentences.append( [w for w in sentence if w not in stop_words] )\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenization to each row of the data frame\n",
    "sentences = tokenize(imdb.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewers',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " 'just',\n",
       " '1',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'will',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly',\n",
       " 'happened',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'oz',\n",
       " 'brutality',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'violence',\n",
       " 'set',\n",
       " 'right',\n",
       " 'word',\n",
       " 'go',\n",
       " 'trust',\n",
       " 'show',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'timid',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'punches',\n",
       " 'regards',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'violence',\n",
       " 'hardcore',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'word',\n",
       " 'called',\n",
       " 'oz',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'oswald',\n",
       " 'maximum',\n",
       " 'security',\n",
       " 'state',\n",
       " 'penitentary',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'emerald',\n",
       " 'city',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'prison',\n",
       " 'cells',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'privacy',\n",
       " 'high',\n",
       " 'agenda',\n",
       " 'em',\n",
       " 'city',\n",
       " 'home',\n",
       " 'many',\n",
       " 'aryans',\n",
       " 'muslims',\n",
       " 'gangstas',\n",
       " 'latinos',\n",
       " 'christians',\n",
       " 'italians',\n",
       " 'irish',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'say',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'show',\n",
       " 'due',\n",
       " 'fact',\n",
       " 'goes',\n",
       " 'shows',\n",
       " 'dare',\n",
       " 'forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romance',\n",
       " 'oz',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'nasty',\n",
       " 'surreal',\n",
       " 'say',\n",
       " 'ready',\n",
       " 'watched',\n",
       " 'developed',\n",
       " 'taste',\n",
       " 'oz',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'just',\n",
       " 'violence',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'will',\n",
       " 'sold',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'will',\n",
       " 'kill',\n",
       " 'order',\n",
       " 'get',\n",
       " 'away',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'turned',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'uncomfortable',\n",
       " 'viewing',\n",
       " 'thats',\n",
       " 'can',\n",
       " 'get',\n",
       " 'touch',\n",
       " 'darker',\n",
       " 'side',\n",
       " '']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     vector_size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70505845, 188819400)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model for 30 epoches on entire dataset\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "word_vectors = Word2Vec.load(\".\\word2vec.model\").wv\n",
    "model =  Word2Vec.load(\".\\word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''in order to feed the data into the Kmeans, created \n",
    "a function to average each row over every single word, as it  \n",
    "is represented by a vector of size vector_size'''\n",
    "\n",
    "import numpy as np\n",
    "def ave_w2v(model, sentences):\n",
    "    ave_f= np.zeros((len(sentences), model.vector_size))\n",
    "    for i, sent in enumerate(sentences):\n",
    "        for word in sent:\n",
    "            try:\n",
    "                vector = model[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        ave_f[i,:] = ave_f[i,:] + vector\n",
    "        ave_f[i,:] = ave_f[i,:] / len(sent)\n",
    "    return ave_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging the words vector across each row\n",
    "ave_w2v = ave_w2v(model, sentences)\n",
    "\n",
    "# KMeans clustering with max_iter 1000\n",
    "from sklearn.cluster import KMeans\n",
    "KMeans = KMeans(n_clusters=2, max_iter=1000, algorithm = 'auto')\n",
    "\n",
    "fitted = KMeans.fit(ave_w2v)\n",
    "prediction = KMeans.predict(ave_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.51\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score\n",
    "imdb['sentiment_pred'] = pd.Series(prediction)\n",
    "f1 = f1_score(imdb['sentiment'],imdb['sentiment_pred'],pos_label=1)\n",
    "acc = accuracy_score(imdb['sentiment'],imdb['sentiment_pred'])\n",
    "\n",
    "print(\"Accuracy {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model performance by trying KMeans for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the cluster centers appropirate values\n",
    "positive = 1\n",
    "positive_center = KMeans.cluster_centers_[positive]\n",
    "negative_center = KMeans.cluster_centers_[1-positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50939, 300)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining the shpae of the vector out of word2vec\n",
    "word_vectors.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.256630</td>\n",
       "      <td>-0.072012</td>\n",
       "      <td>0.077007</td>\n",
       "      <td>0.114384</td>\n",
       "      <td>-0.028004</td>\n",
       "      <td>-0.185907</td>\n",
       "      <td>-0.095212</td>\n",
       "      <td>0.229779</td>\n",
       "      <td>-0.311790</td>\n",
       "      <td>-0.468198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.532663</td>\n",
       "      <td>0.120021</td>\n",
       "      <td>0.128566</td>\n",
       "      <td>-0.123420</td>\n",
       "      <td>0.454978</td>\n",
       "      <td>-0.136039</td>\n",
       "      <td>-0.143117</td>\n",
       "      <td>0.020477</td>\n",
       "      <td>-0.412280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.393435</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>0.059483</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>-0.195026</td>\n",
       "      <td>-0.115573</td>\n",
       "      <td>-0.212024</td>\n",
       "      <td>0.826557</td>\n",
       "      <td>0.112265</td>\n",
       "      <td>-0.020417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291248</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.644949</td>\n",
       "      <td>0.510316</td>\n",
       "      <td>0.302584</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>-0.370578</td>\n",
       "      <td>0.277162</td>\n",
       "      <td>-0.087903</td>\n",
       "      <td>-0.399293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.321017</td>\n",
       "      <td>0.398134</td>\n",
       "      <td>-0.613452</td>\n",
       "      <td>0.271376</td>\n",
       "      <td>-0.426472</td>\n",
       "      <td>-0.106868</td>\n",
       "      <td>-0.118872</td>\n",
       "      <td>0.365422</td>\n",
       "      <td>0.331852</td>\n",
       "      <td>0.085577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085237</td>\n",
       "      <td>0.028916</td>\n",
       "      <td>0.472266</td>\n",
       "      <td>-0.004378</td>\n",
       "      <td>-0.037832</td>\n",
       "      <td>0.341157</td>\n",
       "      <td>-0.043356</td>\n",
       "      <td>0.547153</td>\n",
       "      <td>0.121381</td>\n",
       "      <td>-0.305124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.038850</td>\n",
       "      <td>-0.062278</td>\n",
       "      <td>-0.090060</td>\n",
       "      <td>-0.303378</td>\n",
       "      <td>-0.479485</td>\n",
       "      <td>-0.075731</td>\n",
       "      <td>-0.313882</td>\n",
       "      <td>0.413343</td>\n",
       "      <td>0.344797</td>\n",
       "      <td>-0.279257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141400</td>\n",
       "      <td>0.044314</td>\n",
       "      <td>0.507342</td>\n",
       "      <td>0.210360</td>\n",
       "      <td>0.454749</td>\n",
       "      <td>0.480581</td>\n",
       "      <td>-0.061156</td>\n",
       "      <td>0.357697</td>\n",
       "      <td>0.135310</td>\n",
       "      <td>-0.096451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.375018</td>\n",
       "      <td>0.580950</td>\n",
       "      <td>-0.019273</td>\n",
       "      <td>-0.160223</td>\n",
       "      <td>0.465614</td>\n",
       "      <td>-0.323928</td>\n",
       "      <td>-0.545703</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>-0.254073</td>\n",
       "      <td>-0.044860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301077</td>\n",
       "      <td>0.014529</td>\n",
       "      <td>0.155383</td>\n",
       "      <td>0.098041</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>-0.128783</td>\n",
       "      <td>0.061227</td>\n",
       "      <td>-0.272371</td>\n",
       "      <td>-0.200854</td>\n",
       "      <td>0.195803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.256630 -0.072012  0.077007  0.114384 -0.028004 -0.185907 -0.095212   \n",
       "1 -0.393435  0.664386  0.059483  0.066300 -0.195026 -0.115573 -0.212024   \n",
       "2 -0.321017  0.398134 -0.613452  0.271376 -0.426472 -0.106868 -0.118872   \n",
       "3 -0.038850 -0.062278 -0.090060 -0.303378 -0.479485 -0.075731 -0.313882   \n",
       "4 -0.375018  0.580950 -0.019273 -0.160223  0.465614 -0.323928 -0.545703   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  0.229779 -0.311790 -0.468198  ...  0.012735  0.532663  0.120021  0.128566   \n",
       "1  0.826557  0.112265 -0.020417  ...  0.291248  0.385556  0.644949  0.510316   \n",
       "2  0.365422  0.331852  0.085577  ... -0.085237  0.028916  0.472266 -0.004378   \n",
       "3  0.413343  0.344797 -0.279257  ... -0.141400  0.044314  0.507342  0.210360   \n",
       "4  0.632400 -0.254073 -0.044860  ...  0.301077  0.014529  0.155383  0.098041   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.123420  0.454978 -0.136039 -0.143117  0.020477 -0.412280  \n",
       "1  0.302584  0.324675 -0.370578  0.277162 -0.087903 -0.399293  \n",
       "2 -0.037832  0.341157 -0.043356  0.547153  0.121381 -0.305124  \n",
       "3  0.454749  0.480581 -0.061156  0.357697  0.135310 -0.096451  \n",
       "4  0.049482 -0.128783  0.061227 -0.272371 -0.200854  0.195803  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating words datafame from word_vector that is created from word2vec\n",
    "words = pd.DataFrame(word_vectors.vectors)\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the key for each word, and putting all vectors in one column for each word\n",
    "words['words'] = word_vectors.index_to_key\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "\n",
    "# using the KMeans predict each word cluster and assigning 1 or -1 for each cluster\n",
    "words['cluster'] = words.vectors.apply(lambda x: KMeans.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])\n",
    "words['cluster_number'] = [1 if i==positive  else -1 for i in words.cluster]\n",
    "\n",
    "# kmeans.transform(X) returns is already the L2 norm distance to each cluster center,thus a measure of how \n",
    "# accurate or closeness the word to the cluster\n",
    "words['l2_distance'] = words.apply(lambda x: 1/(KMeans.transform([x.vectors]).min()), axis=1)\n",
    "\n",
    "# calculating score for each word based on their distance to the center, negative number will \n",
    "# be from cluster 0 and positive number from cluster 1\n",
    "words['word_score'] = words.l2_distance * words.cluster_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>l2_distance</th>\n",
       "      <th>word_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.384402</td>\n",
       "      <td>0.323972</td>\n",
       "      <td>0.018749</td>\n",
       "      <td>0.517974</td>\n",
       "      <td>-0.128163</td>\n",
       "      <td>-0.113358</td>\n",
       "      <td>0.091611</td>\n",
       "      <td>0.294123</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>-0.306493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058121</td>\n",
       "      <td>-0.062115</td>\n",
       "      <td>0.125119</td>\n",
       "      <td>-0.604672</td>\n",
       "      <td></td>\n",
       "      <td>[-0.38440162, 0.3239717, 0.018748954, 0.517973...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.243873</td>\n",
       "      <td>0.243873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.385377</td>\n",
       "      <td>0.796786</td>\n",
       "      <td>0.184321</td>\n",
       "      <td>0.218950</td>\n",
       "      <td>-0.170296</td>\n",
       "      <td>0.079719</td>\n",
       "      <td>-0.267830</td>\n",
       "      <td>0.676184</td>\n",
       "      <td>0.250768</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213938</td>\n",
       "      <td>0.289156</td>\n",
       "      <td>-0.256425</td>\n",
       "      <td>-0.893538</td>\n",
       "      <td>movie</td>\n",
       "      <td>[-0.38537654, 0.79678607, 0.18432134, 0.218949...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.167779</td>\n",
       "      <td>0.167779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.214271</td>\n",
       "      <td>0.417396</td>\n",
       "      <td>-0.340524</td>\n",
       "      <td>0.057149</td>\n",
       "      <td>-0.294850</td>\n",
       "      <td>0.101731</td>\n",
       "      <td>-0.122862</td>\n",
       "      <td>0.533590</td>\n",
       "      <td>0.166588</td>\n",
       "      <td>-0.329179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181697</td>\n",
       "      <td>0.639468</td>\n",
       "      <td>-0.335981</td>\n",
       "      <td>-0.638485</td>\n",
       "      <td>film</td>\n",
       "      <td>[-0.21427089, 0.41739598, -0.34052396, 0.05714...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.193551</td>\n",
       "      <td>0.193551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.091152</td>\n",
       "      <td>-0.034703</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.215078</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.221269</td>\n",
       "      <td>-0.246913</td>\n",
       "      <td>0.225454</td>\n",
       "      <td>0.260562</td>\n",
       "      <td>-0.211778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212288</td>\n",
       "      <td>0.098784</td>\n",
       "      <td>-0.195181</td>\n",
       "      <td>-0.585313</td>\n",
       "      <td>one</td>\n",
       "      <td>[-0.091151565, -0.03470321, -0.07899916, -0.21...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.224714</td>\n",
       "      <td>0.224714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.456848</td>\n",
       "      <td>0.414765</td>\n",
       "      <td>0.127270</td>\n",
       "      <td>-0.324015</td>\n",
       "      <td>0.304075</td>\n",
       "      <td>-0.026840</td>\n",
       "      <td>-0.021117</td>\n",
       "      <td>0.417057</td>\n",
       "      <td>-0.468053</td>\n",
       "      <td>-0.205694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185872</td>\n",
       "      <td>-0.014593</td>\n",
       "      <td>-0.126530</td>\n",
       "      <td>0.136474</td>\n",
       "      <td>like</td>\n",
       "      <td>[-0.4568476, 0.41476494, 0.12726973, -0.324014...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196834</td>\n",
       "      <td>0.196834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.384402  0.323972  0.018749  0.517974 -0.128163 -0.113358  0.091611   \n",
       "1 -0.385377  0.796786  0.184321  0.218950 -0.170296  0.079719 -0.267830   \n",
       "2 -0.214271  0.417396 -0.340524  0.057149 -0.294850  0.101731 -0.122862   \n",
       "3 -0.091152 -0.034703 -0.078999 -0.215078  0.003923  0.221269 -0.246913   \n",
       "4 -0.456848  0.414765  0.127270 -0.324015  0.304075 -0.026840 -0.021117   \n",
       "\n",
       "          7         8         9  ...       296       297       298       299  \\\n",
       "0  0.294123  0.084000 -0.306493  ... -0.058121 -0.062115  0.125119 -0.604672   \n",
       "1  0.676184  0.250768  0.021290  ... -0.213938  0.289156 -0.256425 -0.893538   \n",
       "2  0.533590  0.166588 -0.329179  ...  0.181697  0.639468 -0.335981 -0.638485   \n",
       "3  0.225454  0.260562 -0.211778  ... -0.212288  0.098784 -0.195181 -0.585313   \n",
       "4  0.417057 -0.468053 -0.205694  ... -0.185872 -0.014593 -0.126530  0.136474   \n",
       "\n",
       "   words                                            vectors  cluster  \\\n",
       "0         [-0.38440162, 0.3239717, 0.018748954, 0.517973...        1   \n",
       "1  movie  [-0.38537654, 0.79678607, 0.18432134, 0.218949...        1   \n",
       "2   film  [-0.21427089, 0.41739598, -0.34052396, 0.05714...        1   \n",
       "3    one  [-0.091151565, -0.03470321, -0.07899916, -0.21...        1   \n",
       "4   like  [-0.4568476, 0.41476494, 0.12726973, -0.324014...        1   \n",
       "\n",
       "   cluster_number  l2_distance  word_score  \n",
       "0               1     0.243873    0.243873  \n",
       "1               1     0.167779    0.167779  \n",
       "2               1     0.193551    0.193551  \n",
       "3               1     0.224714    0.224714  \n",
       "4               1     0.196834    0.196834  \n",
       "\n",
       "[5 rows x 306 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retaining the needed columns and store them into a new data frame\n",
    "words_trimmed = words[['words', 'vectors', 'cluster', 'cluster_number','l2_distance', 'word_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the words columns as index\n",
    "words_trimmed.set_index(words.words, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>l2_distance</th>\n",
       "      <th>word_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td>[-0.25662994, -0.07201213, 0.07700688, 0.11438...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247434</td>\n",
       "      <td>0.247434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>movie</td>\n",
       "      <td>[-0.39343548, 0.66438645, 0.05948274, 0.066300...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.165659</td>\n",
       "      <td>0.165659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>film</td>\n",
       "      <td>[-0.3210167, 0.39813447, -0.6134517, 0.2713761...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.197589</td>\n",
       "      <td>0.197589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>one</td>\n",
       "      <td>[-0.03885012, -0.062278494, -0.090060316, -0.3...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.223871</td>\n",
       "      <td>0.223871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>like</td>\n",
       "      <td>[-0.37501764, 0.58095044, -0.01927281, -0.1602...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.197565</td>\n",
       "      <td>0.197565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>just</td>\n",
       "      <td>[-0.3624778, 0.69185585, 0.11682977, -0.361674...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192452</td>\n",
       "      <td>0.192452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>good</td>\n",
       "      <td>[0.20961219, 0.54847515, 0.17450707, 0.2739509...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175222</td>\n",
       "      <td>0.175222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>time</td>\n",
       "      <td>[-0.26528504, 0.103988476, -0.16395175, -0.473...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177211</td>\n",
       "      <td>0.177211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>even</td>\n",
       "      <td>[-0.46085772, 0.59741426, -0.38659415, -0.2617...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.209586</td>\n",
       "      <td>0.209586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>will</td>\n",
       "      <td>[-0.6121801, 0.40187812, 0.3320719, 0.2767503,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.157350</td>\n",
       "      <td>0.157350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>story</td>\n",
       "      <td>[0.3611208, 0.5672494, 0.3704282, 0.41895255, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.147401</td>\n",
       "      <td>0.147401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>really</td>\n",
       "      <td>[-0.2458327, 0.22973602, -0.26313314, -0.01748...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.182245</td>\n",
       "      <td>0.182245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>see</td>\n",
       "      <td>[-0.6051536, 0.373089, 0.14823967, 0.2596517, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192449</td>\n",
       "      <td>0.192449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>can</td>\n",
       "      <td>[-0.35491458, 0.54581374, -0.18710367, -0.2845...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.182662</td>\n",
       "      <td>0.182662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>well</td>\n",
       "      <td>[0.12848626, 0.17413588, 0.022163281, 0.076384...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.172414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words                                            vectors  cluster  \\\n",
       "words                                                                        \n",
       "                [-0.25662994, -0.07201213, 0.07700688, 0.11438...        1   \n",
       "movie    movie  [-0.39343548, 0.66438645, 0.05948274, 0.066300...        1   \n",
       "film      film  [-0.3210167, 0.39813447, -0.6134517, 0.2713761...        1   \n",
       "one        one  [-0.03885012, -0.062278494, -0.090060316, -0.3...        1   \n",
       "like      like  [-0.37501764, 0.58095044, -0.01927281, -0.1602...        1   \n",
       "just      just  [-0.3624778, 0.69185585, 0.11682977, -0.361674...        1   \n",
       "good      good  [0.20961219, 0.54847515, 0.17450707, 0.2739509...        1   \n",
       "time      time  [-0.26528504, 0.103988476, -0.16395175, -0.473...        1   \n",
       "even      even  [-0.46085772, 0.59741426, -0.38659415, -0.2617...        1   \n",
       "will      will  [-0.6121801, 0.40187812, 0.3320719, 0.2767503,...        1   \n",
       "story    story  [0.3611208, 0.5672494, 0.3704282, 0.41895255, ...        1   \n",
       "really  really  [-0.2458327, 0.22973602, -0.26313314, -0.01748...        1   \n",
       "see        see  [-0.6051536, 0.373089, 0.14823967, 0.2596517, ...        1   \n",
       "can        can  [-0.35491458, 0.54581374, -0.18710367, -0.2845...        1   \n",
       "well      well  [0.12848626, 0.17413588, 0.022163281, 0.076384...        1   \n",
       "\n",
       "        cluster_number  l2_distance  word_score  \n",
       "words                                            \n",
       "                     1     0.247434    0.247434  \n",
       "movie                1     0.165659    0.165659  \n",
       "film                 1     0.197589    0.197589  \n",
       "one                  1     0.223871    0.223871  \n",
       "like                 1     0.197565    0.197565  \n",
       "just                 1     0.192452    0.192452  \n",
       "good                 1     0.175222    0.175222  \n",
       "time                 1     0.177211    0.177211  \n",
       "even                 1     0.209586    0.209586  \n",
       "will                 1     0.157350    0.157350  \n",
       "story                1     0.147401    0.147401  \n",
       "really               1     0.182245    0.182245  \n",
       "see                  1     0.192449    0.192449  \n",
       "can                  1     0.182662    0.182662  \n",
       "well                 1     0.172414    0.172414  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_trimmed.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating an average score for each row based on its words scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = np.zeros(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function iterate through each sentence, and grabs their word score as defined above\n",
    "# then average over the length of that sentence to calculate a value which is an average score value\n",
    "# of all the words\n",
    "for i, row in enumerate(sentences):\n",
    "    if i%100 == 0:\n",
    "        print('iteration ', i)\n",
    "    row_list = []\n",
    "    for sent in row:\n",
    "        if words_trimmed['words'].str.contains(sent).any():\n",
    "            try:\n",
    "                row_list.append(words_trimmed.loc[sent]['word_score'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "    new_pred[i] = (np.mean(row_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the predictoin into a new column\n",
    "imdb['new_pred']=pd.Series(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb['new_pred'] = [1 if i > 0 else 0 for i in imdb.new_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_final = imdb.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score\n",
    "f1 = f1_score(imdb_final['sentiment'],imdb_final['new_pred'],pos_label=1)\n",
    "acc = accuracy_score(imdb_final['sentiment'],imdb_final['new_pred'])\n",
    "\n",
    "print(\"Accuracy {:.2f}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
