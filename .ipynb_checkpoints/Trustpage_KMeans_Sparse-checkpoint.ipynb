{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Sentiment Analysis - KMeans Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '.\\IMDB_Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.split()\n",
    "    text = str(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping NAN and duplicates from the data\n",
    "imdb_trimmed = imdb.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the text prep to the review column\n",
    "imdb_trimmed.review = imdb_trimmed.review.apply(lambda x: text_to_word_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['one', 'of', 'the', 'other', 'reviewers', 'ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['a', 'wonderful', 'little', 'production', 'th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['i', 'thought', 'this', 'was', 'a', 'wonderfu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['basically', 'there', 'a', 'family', 'where',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['petter', 'mattei', 'love', 'in', 'the', 'tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  ['one', 'of', 'the', 'other', 'reviewers', 'ha...          1\n",
       "1  ['a', 'wonderful', 'little', 'production', 'th...          1\n",
       "2  ['i', 'thought', 'this', 'was', 'a', 'wonderfu...          1\n",
       "3  ['basically', 'there', 'a', 'family', 'where',...          0\n",
       "4  ['petter', 'mattei', 'love', 'in', 'the', 'tim...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating label for sentiment for evaluation\n",
    "imdb_trimmed['sentiment'] = imdb_trimmed['sentiment'].map({'positive':1,'negative':0})\n",
    "imdb_trimmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - Applying KMeans Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing, normalizing, and applying TruncatedSVD to reduce the dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from text using bag of words - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizng the texts\n",
    "vectorizer = CountVectorizer(stop_words = 'english',max_features=20000)\n",
    "vec = vectorizer.fit_transform(imdb_trimmed.review)\n",
    "\n",
    "# normlaizing the vectorized vector\n",
    "vec_norm = normalize(vec)\n",
    "vec_arr = vec_norm.toarray()\n",
    "\n",
    "#performing TruncatedSVD to reduce dimensionality\n",
    "vec_TSVD = TruncatedSVD(n_components = 100)\n",
    "vec_tranformed = vec_TSVD.fit_transform(vec_arr)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, max_iter=1000, algorithm = 'auto')\n",
    "\n",
    "fitted = kmeans.fit(vec)\n",
    "prediction = kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.26 and Accuracy 0.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "imdb_trimmed['sentiment_pred_vec'] = pd.Series(prediction)\n",
    "f1 = f1_score(imdb_trimmed['sentiment'],imdb_trimmed['sentiment_pred_vec'],pos_label=1)\n",
    "acc = accuracy_score(imdb_trimmed['sentiment'],imdb_trimmed['sentiment_pred_vec'])\n",
    "\n",
    "print(\"F1 Score : {:.2f} and Accuracy {:.2f}\".format(f1,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kills</th>\n",
       "      <th>kilmer</th>\n",
       "      <th>kilter</th>\n",
       "      <th>kim</th>\n",
       "      <th>kimberly</th>\n",
       "      <th>kin</th>\n",
       "      <th>kind</th>\n",
       "      <th>kinda</th>\n",
       "      <th>kindergarten</th>\n",
       "      <th>kindly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kills  kilmer  kilter  kim  kimberly  kin  kind  kinda  kindergarten  \\\n",
       "0      0       0       0    0         0    0     0      0             0   \n",
       "1      0       0       0    0         0    0     0      0             0   \n",
       "2      0       0       0    0         0    0     0      0             0   \n",
       "3      0       0       0    0         0    0     0      0             0   \n",
       "4      0       0       0    0         0    0     0      0             0   \n",
       "5      0       0       0    0         0    0     1      0             0   \n",
       "6      0       0       0    0         0    0     0      0             0   \n",
       "7      0       0       0    0         0    0     0      0             0   \n",
       "8      0       0       0    0         0    0     0      0             0   \n",
       "9      0       0       0    0         0    0     0      0             0   \n",
       "\n",
       "   kindly  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "5       0  \n",
       "6       0  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizing the sparse matrix\n",
    "i = 10000\n",
    "j = 10\n",
    "words = vectorizer.get_feature_names()[i:i+10]\n",
    "pd.DataFrame(vec[j:j+10,i:i+10].todense(), columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from text using bag of words - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.39 GiB for an array with shape (49582, 20000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2384/2187308715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvec_norm_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_tf_idf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mvec_arr_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_norm_tf_idf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvec_TSVD_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.39 GiB for an array with shape (49582, 20000) and data type float64"
     ]
    }
   ],
   "source": [
    "# applying the KMeans with tf-idf \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tf_idf = TfidfVectorizer(stop_words = 'english',max_features=20000)\n",
    "vec_tf_idf = vectorizer_tf_idf.fit_transform(imdb_trimmed.review)\n",
    "\n",
    "vec_norm_tf_idf = normalize(vec_tf_idf)\n",
    "vec_arr_tf_idf = vec_norm_tf_idf.toarray()\n",
    "\n",
    "vec_TSVD_tf_idf = TruncatedSVD(n_components = 100)\n",
    "vec_tranformed_tf_idf = vec_TSVD_tf_idf.fit_transform(vec_arr_tf_idf)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, max_iter=1000, algorithm = 'auto')\n",
    "\n",
    "fitted_tf_idf = kmeans.fit(vec_tf_idf)\n",
    "prediction_tf_idf = kmeans.predict(vec_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.65 and Accuracy 0.57\n"
     ]
    }
   ],
   "source": [
    "imdb_trimmed['sentiment_pred_tf_idf'] = pd.Series(prediction_tf_idf)\n",
    "f1 = f1_score(imdb_trimmed['sentiment'],imdb_trimmed['sentiment_pred_tf_idf'],pos_label=1)\n",
    "acc = accuracy_score(imdb_trimmed['sentiment'],imdb_trimmed['sentiment_pred_tf_idf'])\n",
    "\n",
    "print(\"F1 Score : {:.2f} and Accuracy {:.2f}\".format(f1,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing combination of different models - 27 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tune hyperparameters, created a function to apply changing parameters and resulting different models. Three parameters that can be changed are max_iter, max_features, min_df=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_param_vec(max_iter=1000, max_features=None, min_df=1):\n",
    "  vectorizer = CountVectorizer(max_features=max_features, min_df=min_df)\n",
    "  features = vectorizer.fit_transform(imdb_trimmed.review)\n",
    "  model = KMeans(n_clusters=2, max_iter=max_iter, algorithm = 'auto')\n",
    "  model.fit(features)\n",
    "  pred = model.predict(features)\n",
    "  return pred, {\n",
    "    \"max_features\": max_features,\n",
    "    \"min_df\": min_df,\n",
    "    \"max_iter\": max_iter,\n",
    "    \"auc\": roc_auc_score(imdb_trimmed.sentiment, pred)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def model_param_tf_id(max_iter=1000, max_features=None, min_df=1):\n",
    "  vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df)\n",
    "  features = vectorizer.fit_transform(imdb_trimmed.review)\n",
    "  model = KMeans(n_clusters=2, max_iter=max_iter, algorithm = 'auto')\n",
    "  model.fit(features)\n",
    "  pred = model.predict(features)\n",
    "  return {\n",
    "    \"max_features\": max_features,\n",
    "    \"min_df\": min_df,\n",
    "    \"max_iter\": max_iter,\n",
    "    \"auc\": roc_auc_score(imdb_trimmed.sentiment, pred),\n",
    "    \"acc\": accuracy_score(imdb_trimmed.sentiment, pred)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "params = {\n",
    "  \"max_features\": [10000, 50000, None],\n",
    "  \"min_df\": [1,2,3],\n",
    "  \"max_iter\": [1000, 2000, 3000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 10000, 'min_df': 1, 'max_iter': 1000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 10000, 'min_df': 1, 'max_iter': 2000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 10000, 'min_df': 1, 'max_iter': 3000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': 10000, 'min_df': 2, 'max_iter': 1000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 10000, 'min_df': 2, 'max_iter': 2000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 10000, 'min_df': 2, 'max_iter': 3000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 10000, 'min_df': 3, 'max_iter': 1000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': 10000, 'min_df': 3, 'max_iter': 2000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 10000, 'min_df': 3, 'max_iter': 3000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': 50000, 'min_df': 1, 'max_iter': 1000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 50000, 'min_df': 1, 'max_iter': 2000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 50000, 'min_df': 1, 'max_iter': 3000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 50000, 'min_df': 2, 'max_iter': 1000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': 50000, 'min_df': 2, 'max_iter': 2000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 50000, 'min_df': 2, 'max_iter': 3000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 50000, 'min_df': 3, 'max_iter': 1000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': 50000, 'min_df': 3, 'max_iter': 2000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': 50000, 'min_df': 3, 'max_iter': 3000, 'auc': 0.5096116740441541})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': None, 'min_df': 1, 'max_iter': 1000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': None, 'min_df': 1, 'max_iter': 2000, 'auc': 0.4903883259558459})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': None, 'min_df': 1, 'max_iter': 3000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': None, 'min_df': 2, 'max_iter': 1000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': None, 'min_df': 2, 'max_iter': 2000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': None, 'min_df': 2, 'max_iter': 3000, 'auc': 0.5096116740441541})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': None, 'min_df': 3, 'max_iter': 1000, 'auc': 0.4903883259558459})\n",
      "(array([1, 1, 1, ..., 1, 1, 1]), {'max_features': None, 'min_df': 3, 'max_iter': 2000, 'auc': 0.4903883259558459})\n",
      "(array([0, 0, 0, ..., 0, 0, 0]), {'max_features': None, 'min_df': 3, 'max_iter': 3000, 'auc': 0.5096116740441541})\n"
     ]
    }
   ],
   "source": [
    "results_vec = []\n",
    "for p in product(*params.values()):\n",
    "  res = model_param_vec(**dict(zip(params.keys(), p)))\n",
    "  results_vec.append( res )\n",
    "  print (res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 10000, 'min_df': 1, 'max_iter': 1000, 'auc': 0.5045016960321936, 'acc': 0.5042959138396998}\n",
      "{'max_features': 10000, 'min_df': 1, 'max_iter': 2000, 'auc': 0.5038721476705278, 'acc': 0.5036706869428421}\n",
      "{'max_features': 10000, 'min_df': 1, 'max_iter': 3000, 'auc': 0.4953949626941126, 'acc': 0.4956032431124198}\n",
      "{'max_features': 10000, 'min_df': 2, 'max_iter': 1000, 'auc': 0.5049081003326485, 'acc': 0.504699286031221}\n",
      "{'max_features': 10000, 'min_df': 2, 'max_iter': 2000, 'auc': 0.5048065749183426, 'acc': 0.5045984429833408}\n",
      "{'max_features': 10000, 'min_df': 2, 'max_iter': 3000, 'auc': 0.49519342508165737, 'acc': 0.4954015570166593}\n",
      "{'max_features': 10000, 'min_df': 3, 'max_iter': 1000, 'auc': 0.5045036632131972, 'acc': 0.5042959138396998}\n",
      "{'max_features': 10000, 'min_df': 3, 'max_iter': 2000, 'auc': 0.49628723629572546, 'acc': 0.4964906619337663}\n",
      "{'max_features': 10000, 'min_df': 3, 'max_iter': 3000, 'auc': 0.4954544857838321, 'acc': 0.495663748941148}\n",
      "{'max_features': 50000, 'min_df': 1, 'max_iter': 1000, 'auc': 0.5411604410827889, 'acc': 0.5413254810213384}\n",
      "{'max_features': 50000, 'min_df': 1, 'max_iter': 2000, 'auc': 0.45885980347142585, 'acc': 0.45869468758823767}\n",
      "{'max_features': 50000, 'min_df': 1, 'max_iter': 3000, 'auc': 0.5420327727734182, 'acc': 0.5421927312331087}\n",
      "{'max_features': 50000, 'min_df': 2, 'max_iter': 1000, 'auc': 0.4568936605667286, 'acc': 0.45673833245936024}\n",
      "{'max_features': 50000, 'min_df': 2, 'max_iter': 2000, 'auc': 0.4568933579234972, 'acc': 0.45673833245936024}\n",
      "{'max_features': 50000, 'min_df': 2, 'max_iter': 3000, 'auc': 0.4583834348897713, 'acc': 0.4582308095679884}\n",
      "{'max_features': 50000, 'min_df': 3, 'max_iter': 1000, 'auc': 0.5423134727433453, 'acc': 0.5424750917671736}\n",
      "{'max_features': 50000, 'min_df': 3, 'max_iter': 2000, 'auc': 0.5420144954002069, 'acc': 0.5421725626235327}\n",
      "{'max_features': 50000, 'min_df': 3, 'max_iter': 3000, 'auc': 0.5423134727433453, 'acc': 0.5424750917671736}\n",
      "{'max_features': None, 'min_df': 1, 'max_iter': 1000, 'auc': 0.45615263210640644, 'acc': 0.4559920939050462}\n",
      "{'max_features': None, 'min_df': 1, 'max_iter': 2000, 'auc': 0.45744888886262364, 'acc': 0.45728288491791375}\n",
      "{'max_features': None, 'min_df': 1, 'max_iter': 3000, 'auc': 0.5425511111373764, 'acc': 0.5427171150820862}\n",
      "{'max_features': None, 'min_df': 2, 'max_iter': 1000, 'auc': 0.542736641200855, 'acc': 0.5428986325682708}\n",
      "{'max_features': None, 'min_df': 2, 'max_iter': 2000, 'auc': 0.5427567344334543, 'acc': 0.5429188011778467}\n",
      "{'max_features': None, 'min_df': 2, 'max_iter': 3000, 'auc': 0.4572432655665458, 'acc': 0.4570811988221532}\n",
      "{'max_features': None, 'min_df': 3, 'max_iter': 1000, 'auc': 0.45780617872255636, 'acc': 0.45764591989028275}\n",
      "{'max_features': None, 'min_df': 3, 'max_iter': 2000, 'auc': 0.4576865272566547, 'acc': 0.45752490823282643}\n",
      "{'max_features': None, 'min_df': 3, 'max_iter': 3000, 'auc': 0.5404239522709364, 'acc': 0.5405792424670243}\n"
     ]
    }
   ],
   "source": [
    "results_tf_id = []\n",
    "for p in product(*params.values()):\n",
    "  res = model_param_tf_id(**dict(zip(params.keys(), p)))\n",
    "  results_tf_id.append( res )\n",
    "  print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing TfidfVectorizer - NO TruncatedSVD - Optimized parameters of max_features=None and min_df=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the KMeans with tf-idf _ not performing PCA \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tf_idf = TfidfVectorizer(stop_words = 'english',max_features=None, min_df=2)\n",
    "vec_tf_idf = vectorizer_tf_idf.fit_transform(imdb_trimmed.review)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, max_iter=3000, algorithm = 'auto')\n",
    "\n",
    "fitted_tf_idf = kmeans.fit(vec_tf_idf)\n",
    "prediction_tf_idf = kmeans.predict(vec_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.65 and Accuracy 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score\n",
    "\n",
    "imdb_trimmed['sentiment_pred_tf_idf'] = pd.Series(prediction_tf_idf)\n",
    "f1 = f1_score(imdb_trimmed['sentiment'],imdb_trimmed['sentiment_pred_tf_idf'],pos_label=1)\n",
    "acc = accuracy_score(imdb_trimmed['sentiment'],imdb_trimmed['sentiment_pred_tf_idf'])\n",
    "\n",
    "print(\"F1 Score : {:.2f} and Accuracy {:.2f}\".format(f1,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulizing the results of TfidfVectorizer with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_arr_tf_idf = vec_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.9 GiB for an array with shape (35892, 59392) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2384/1915412334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_arr_tf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_tf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2384/1915412334.py\u001b[0m in \u001b[0;36mtop_cluster\u001b[1;34m(vec_arr, prediction, number_features)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mxmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mmeanavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnumber_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_tf_idf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 15.9 GiB for an array with shape (35892, 59392) and data type float64"
     ]
    }
   ],
   "source": [
    "def top_cluster(vec_arr, prediction, number_features):\n",
    "    labels = np.unique(prediction)\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        idx = np.where(prediction==label)\n",
    "        xmeans = np.mean(vec_arr[idx], axis = 0)\n",
    "        meanavg = np.argsort(xmeans)[::-1][:number_features]\n",
    "        features = vectorizer_tf_idf.get_feature_names()\n",
    "        best_features = [(features[i], xmeans[i]) for i in meanavg]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        data.append(df)\n",
    "    return data\n",
    "words = top_cluster(vec_arr_tf_idf, prediction_tf_idf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = words[0][:15]);\n",
    "plt.title(\"Cluster: \"+ str(0), fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = words[1][:15]);\n",
    "plt.title(\"Cluster: \"+ str(1), fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
